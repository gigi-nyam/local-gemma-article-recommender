"""
ãƒãƒ«ãƒLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œã®è¨˜äº‹æ¨è–¦ãƒ­ã‚¸ãƒƒã‚¯
Ollama (ãƒ­ãƒ¼ã‚«ãƒ«)ã€Geminiã€OpenAI ã«å¯¾å¿œ
"""

import os
import requests
import json
import re
from typing import List, Dict, Optional
from pydantic import BaseModel
from dotenv import load_dotenv
from abc import ABC, abstractmethod

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


class ArticleRecommendation(BaseModel):
    """æ¨è–¦è¨˜äº‹ã®æ§‹é€ åŒ–å‡ºåŠ›"""
    article_id: int
    title: str
    reason: str
    clickbait_score: float  # 0-1ã®ã‚¹ã‚³ã‚¢


class RecommendationResult(BaseModel):
    """æ¨è–¦çµæœã®æ§‹é€ åŒ–å‡ºåŠ›"""
    recommendations: List[ArticleRecommendation]
    reasoning: str


class BaseLLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def generate(self, prompt: str) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        pass


class OllamaProvider(BaseLLMProvider):
    """Ollama API ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ (ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«)"""
    
    def __init__(self, model: str, base_url: str):
        self.model = model
        self.base_url = base_url
        
        # OllamaãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = [m['name'] for m in response.json().get('models', [])]
                if self.model not in models:
                    print(f"è­¦å‘Š: {self.model}ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                    print(f"å®Ÿè¡Œã—ã¦ãã ã•ã„: ollama pull {self.model}")
            else:
                print(f"è­¦å‘Š: Ollama APIã«æ¥ç¶šã§ãã¾ã›ã‚“({base_url})")
        except Exception as e:
            print(f"Ollamaã®ç¢ºèªã«å¤±æ•—: {e}")
    
    def generate(self, prompt: str, timeout: int = 180) -> str:
        url = f"{self.base_url}/api/generate"
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 1000
            }
        }
        
        response = requests.post(url, json=payload, timeout=timeout)
        
        if response.status_code != 200:
            raise RuntimeError(f"Ollama API ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")
        
        result = response.json()
        return result.get('response', '')


class GeminiProvider(BaseLLMProvider):
    """Google Gemini API ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, api_key: str, model: str = "gemini-3-pro-preview"):
        self.api_key = api_key
        self.model = model
        
        try:
            import google.generativeai as genai
            from google.generativeai import types
            
            self.genai = genai
            self.types = types
            
            genai.configure(api_key=api_key)
            self.client = genai.GenerativeModel(model)
            print(f"âœ“ Gemini APIæ¥ç¶šæˆåŠŸ (ãƒ¢ãƒ‡ãƒ«: {model})")
        except ImportError:
            raise ImportError("google-generativeaiãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ã§ã™: pip install google-generativeai")
        except Exception as e:
            print(f"è­¦å‘Š: Gemini APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate(self, prompt: str, timeout: int = 180) -> str:
        try:
            from google.generativeai.types import HarmCategory, HarmBlockThreshold
            from google.generativeai import types
            
            # ç’°å¢ƒå¤‰æ•°ã§thinking_levelã‚’æŒ‡å®šå¯èƒ½ã«ã™ã‚‹
            # GEMINI_THINKING_LEVEL=low/medium/high/none ã§åˆ¶å¾¡
            thinking_level_env = os.getenv("GEMINI_THINKING_LEVEL", "").lower()
            thinking_level = None
            if thinking_level_env and thinking_level_env != "none":
                thinking_level = thinking_level_env
            
            # GenerateContentConfigã®è¨­å®š
            if thinking_level:
                try:
                    config = types.GenerateContentConfig(
                        temperature=0.7,
                        max_output_tokens=4000,
                        thinking_config=types.ThinkingConfig(thinking_level=thinking_level)
                    )
                    print(f"  ğŸ’­ thinking_level={thinking_level} ã‚’é©ç”¨ã—ã¾ã—ãŸ")
                except (TypeError, AttributeError) as e:
                    # thinking_levelãŒã¾ã ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã¯é€šå¸¸ã®è¨­å®šã‚’ä½¿ç”¨
                    print(f"  âš ï¸ thinking_level ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆ{e}ï¼‰")
                    config = types.GenerateContentConfig(
                        temperature=0.7,
                        max_output_tokens=4000
                    )
            else:
                config = types.GenerateContentConfig(
                    temperature=0.7,
                    max_output_tokens=4000
                )
                if thinking_level_env:
                    print(f"  â„¹ï¸ GEMINI_THINKING_LEVEL={thinking_level_env} (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œ)")
                else:
                    print(f"  â„¹ï¸ thinking_level æœªè¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œï¼‰")
            
            response = self.client.generate_content(
                prompt,
                config=config,
                safety_settings={
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒå®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
            if not response.candidates:
                raise RuntimeError("Gemini APIãŒå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆå®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚Šãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")
            
            candidate = response.candidates[0]
            # finish_reason: 1=STOP(æ­£å¸¸), 2=MAX_TOKENS(ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã ãŒå†…å®¹ã¯å–å¾—å¯èƒ½)
            if candidate.finish_reason not in [1, 2]:
                finish_reasons = {
                    0: "FINISH_REASON_UNSPECIFIED",
                    1: "STOP",
                    2: "MAX_TOKENS",
                    3: "SAFETY",
                    4: "RECITATION",
                    5: "OTHER"
                }
                reason = finish_reasons.get(candidate.finish_reason, "UNKNOWN")
                raise RuntimeError(f"Gemini APIãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ: finish_reason={reason}")
            
            # response.textã®ä»£ã‚ã‚Šã«ã€ã‚ˆã‚Šå®‰å…¨ãªæ–¹æ³•ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if candidate.content and candidate.content.parts:
                return "".join(part.text for part in candidate.content.parts if hasattr(part, 'text'))
            else:
                raise RuntimeError(f"Gemini APIã®å¿œç­”ã«ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ (finish_reason={candidate.finish_reason})")
        except Exception as e:
            raise RuntimeError(f"Gemini API ã‚¨ãƒ©ãƒ¼: {e}")


class OpenAIProvider(BaseLLMProvider):
    """OpenAI API ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        self.api_key = api_key
        self.model = model
        
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=api_key)
            print(f"âœ“ OpenAI APIæ¥ç¶šæˆåŠŸ (ãƒ¢ãƒ‡ãƒ«: {model})")
        except ImportError:
            raise ImportError("openaiãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ã§ã™: pip install openai")
        except Exception as e:
            print(f"è­¦å‘Š: OpenAI APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate(self, prompt: str, timeout: int = 180) -> str:
        try:
            # GPT-5ç³»ã§ã¯ max_completion_tokensã€GPT-4ç³»ä»¥å‰ã§ã¯ max_tokens ã‚’ä½¿ç”¨
            params = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "ã‚ãªãŸã¯è¨˜äº‹æ¨è–¦ã®å°‚é–€å®¶ã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ]
            }
            
            # ãƒ¢ãƒ‡ãƒ«åã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ‡ã‚Šæ›¿ãˆ
            if self.model.startswith("gpt-5") or self.model.startswith("o1") or self.model.startswith("o3"):
                params["max_completion_tokens"] = 1000
                # GPT-5ç³»ã¯temperatureã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤(1.0)ã®ã¿ã‚µãƒãƒ¼ãƒˆ
            else:
                params["max_tokens"] = 1000
                params["temperature"] = 0.7
            
            response = self.client.chat.completions.create(**params)
            return response.choices[0].message.content
        except Exception as e:
            raise RuntimeError(f"OpenAI API ã‚¨ãƒ©ãƒ¼: {e}")


class LocalGemmaRecommender:
    """ãƒãƒ«ãƒLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œã®è¨˜äº‹æ¨è–¦ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(
        self, 
        provider: Optional[str] = None,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            provider: LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ ("ollama", "gemini", "openai")ã€‚Noneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åã€‚Noneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
            api_key: APIã‚­ãƒ¼ã€‚Noneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
            base_url: Ollama APIã®ãƒ™ãƒ¼ã‚¹URL (Ollamaã®å ´åˆã®ã¿)
        """
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
        provider = provider or os.getenv("LLM_PROVIDER", "ollama")
        
        self.provider_name = provider
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        if provider == "ollama":
            model = model or os.getenv("OLLAMA_MODEL", "gemma3:4b")
            base_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            self.llm_provider = OllamaProvider(model=model, base_url=base_url)
            print(f"âœ“ Ollamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨ (ãƒ¢ãƒ‡ãƒ«: {model})")
            
        elif provider == "gemini":
            api_key = api_key or os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            model = model or os.getenv("GEMINI_MODEL", "gemini-3-pro-preview")
            self.llm_provider = GeminiProvider(api_key=api_key, model=model)
            print(f"âœ“ Geminiãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨ (ãƒ¢ãƒ‡ãƒ«: {model})")
            
        elif provider == "openai":
            api_key = api_key or os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            model = model or os.getenv("OPENAI_MODEL", "gpt-5.1")
            self.llm_provider = OpenAIProvider(api_key=api_key, model=model)
            print(f"âœ“ OpenAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨ (ãƒ¢ãƒ‡ãƒ«: {model})")
            
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")
    
    def recommend_articles(
        self,
        user_query: str,
        candidate_articles: List[Dict],
        top_k: int = 3
    ) -> RecommendationResult:
        """
        å€™è£œè¨˜äº‹ã‹ã‚‰ã€Œã¤ã„ã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ã€è¨˜äº‹ã‚’é¸æŠ
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¤œç´¢ã‚¯ã‚¨ãƒª
            candidate_articles: å€™è£œè¨˜äº‹ã®ãƒªã‚¹ãƒˆ
            top_k: æ¨è–¦ã™ã‚‹è¨˜äº‹æ•°
            
        Returns:
            æ¨è–¦çµæœ
        """
        # å€™è£œè¨˜äº‹ã‚’çµã‚Šè¾¼ã‚€ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã™ãã‚‹ãŸã‚ï¼‰
        limited_candidates = candidate_articles[:min(10, len(candidate_articles))]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = self._build_prompt(user_query, limited_candidates, top_k)
        
        # LLM APIã§æ¨è«–ã‚’å®Ÿè¡Œ
        try:
            response_text = self.llm_provider.generate(prompt)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = self._parse_response(response_text, limited_candidates)
            
            return result
            
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®top_kä»¶ã‚’è¿”ã™
            return self._fallback_recommendation(limited_candidates, top_k)
    
    def _build_prompt(
        self,
        user_query: str,
        candidate_articles: List[Dict],
        top_k: int
    ) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆç°¡æ½”ç‰ˆï¼‰"""
        articles_text = "\n".join([
            # f"ID:{article['id']} {article['title']}"
            f"ID:{article['id']} [{article['summary']}] {article['title']}"
            for article in candidate_articles
        ])
        
        prompt = f"""ã‚ãªãŸã¯è¨˜äº‹æ¨è–¦ã®å°‚é–€å®¶ã§ã™ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª: ã€Œ{user_query}ã€

ä»¥ä¸‹ã®å€™è£œè¨˜äº‹ã‹ã‚‰ã€Œã¤ã„ã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ã€è¨˜äº‹ã‚’{top_k}ä»¶é¸ã‚“ã§ãã ã•ã„ã€‚
ãŸã ã—ã€å…¥åŠ›ã¨ä¼¼ãŸè¨˜äº‹ã‚„ä¸€èˆ¬çš„ãªäººæ°—è¨˜äº‹ã¯é¿ã‘ã¦ã€ã“ã®è¨˜äº‹ã‚’èª­ã‚“ã å¾Œã§æ°—ã«ãªã‚Šãã†ãªè¨˜äº‹ã«é™ã£ã¦ãã ã•ã„ã€‚

å€™è£œè¨˜äº‹:
{articles_text}

ä»¥ä¸‹ã®æœ‰åŠ¹ãªJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
é‡è¦: titleã«ã¯è¨˜äº‹ã®ç°¡æ½”ãªè¦ç´„ã‚’æ›¸ã„ã¦ãã ã•ã„ï¼ˆå…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼ã—ãªã„ï¼‰ã€‚
é‡è¦: æ–‡å­—åˆ—å€¤ã¯å¿…ãšãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ"ï¼‰ã§å›²ã‚“ã§ãã ã•ã„ã€‚

{{
  "recommendations": [
    {{"article_id": 2, "title": "è¨˜äº‹ã®è¦ç´„", "reason": "é¸æŠç†ç”±", "clickbait_score": 0.85}}
  ],
  "reasoning": "é¸æŠæ–¹é‡"
}}

å¿…ãšæœ‰åŠ¹ãªJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚"""
        
        return prompt
    
    def _parse_response(
        self,
        response_text: str,
        candidate_articles: List[Dict]
    ) -> RecommendationResult:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            json_match = re.search(r'```json\s*\n(.+?)\n```', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                json_match = re.search(r'\{.*"recommendations".*\}', response_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(0)
                else:
                    # å…¨ä½“ã‚’JSONã¨ã—ã¦è©¦ã™
                    json_text = response_text.strip()
            
            # ä½™è¨ˆãªæ–‡å­—ã‚’å‰Šé™¤
            json_text = json_text.strip()
            
            # JSONå†…ã®å¼•ç”¨ç¬¦ã®å•é¡Œã‚’ä¿®æ­£ï¼ˆ"ãŒ"ã«ãªã£ã¦ã„ã‚‹å ´åˆãªã©ï¼‰
            # ä¸å®Œå…¨ãªJSONã‚’ä¿®æ­£
            if not json_text.endswith('}'):
                # æœ€å¾Œã®å®Œå…¨ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¾ã§åˆ‡ã‚Šå–ã‚‹
                last_brace = json_text.rfind('}')
                if last_brace > 0:
                    json_text = json_text[:last_brace + 1]
            
            # GemmaãŒé…åˆ—ã®è¦ç´ é–“ã®ã‚«ãƒ³ãƒã‚’çœç•¥ã™ã‚‹å•é¡Œã‚’ä¿®æ­£
            # ä¾‹: }{  â†’  },{  
            json_text = re.sub(r'\}\s*\n\s*\{', '},\n{', json_text)
            
            data = json.loads(json_text)
            
            # Pydanticãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›
            recommendations = []
            for rec in data.get('recommendations', []):
                # è¨˜äº‹IDã«å¯¾å¿œã™ã‚‹å®Œå…¨ãªæƒ…å ±ã‚’å–å¾—
                article_id = rec.get('article_id')
                article = next((a for a in candidate_articles if a['id'] == article_id), None)
                
                if article:
                    recommendations.append(ArticleRecommendation(
                        article_id=article_id,
                        title=rec.get('title', article['title']),
                        reason=rec.get('reason', ''),
                        clickbait_score=rec.get('clickbait_score', 0.5)
                    ))
            
            return RecommendationResult(
                recommendations=recommendations,
                reasoning=data.get('reasoning', '')
            )
                
        except Exception as e:
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {e}")
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰: {response_text[:500]}")
            raise
    
    def _fallback_recommendation(
        self,
        candidate_articles: List[Dict],
        top_k: int
    ) -> RecommendationResult:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¨è–¦"""
        recommendations = [
            ArticleRecommendation(
                article_id=article['id'],
                title=article['title'],
                reason="ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨è–¦",
                clickbait_score=0.5
            )
            for article in candidate_articles[:top_k]
        ]
        
        return RecommendationResult(
            recommendations=recommendations,
            reasoning="ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨è–¦ã‚’ä½¿ç”¨"
        )


def demo_local_gemma_recommender():
    """ãƒãƒ«ãƒLLMæ¨è–¦ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    from sample_articles import SAMPLE_ARTICLES
    
    print("=" * 60)
    print("ãƒãƒ«ãƒLLMè¨˜äº‹æ¨è–¦ãƒ‡ãƒ¢")
    print("=" * 60)
    print()
    
    # æ¨è–¦ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ– (ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿)
    recommender = LocalGemmaRecommender()
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "å²©æ‰‹ã®ä½å®…åœ°è¿‘ãã§ã‚¯ãƒ2é ­ãŒé€£æ—¥æŸ¿ã®æœ¨ã«å‡ºæ²¡",
    ]
    
    for test_query in test_queries:
        print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª: ã€Œ{test_query}ã€")
        print("-" * 60)
        print()
        
        # å€™è£œè¨˜äº‹
        candidate_articles = SAMPLE_ARTICLES
        
        print(f"å€™è£œè¨˜äº‹æ•°: {len(candidate_articles)}ä»¶ï¼ˆä¸Šä½10ä»¶ã«çµã‚Šè¾¼ã¿ï¼‰")
        print(f"{recommender.provider_name}ã§ã€Œã¤ã„ã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ã€è¨˜äº‹ã‚’3ä»¶é¸æŠä¸­...")
        print()
        
        # æ¨è–¦ã‚’å®Ÿè¡Œ
        import time
        start_time = time.time()
        
        result = recommender.recommend_articles(
            user_query=test_query,
            candidate_articles=candidate_articles,
            top_k=3
        )
        
        elapsed_time = time.time() - start_time
        
        # çµæœã‚’è¡¨ç¤º
        print("ã€æ¨è–¦çµæœã€‘")
        print(f"é¸æŠæ–¹é‡: {result.reasoning}")
        print()
        
        for i, rec in enumerate(result.recommendations, 1):
            print(f"{i}. {rec.title}")
            print(f"   ã‚¯ãƒªãƒƒã‚¯èª˜å¼•åº¦: {rec.clickbait_score:.2f}")
            print(f"   é¸æŠç†ç”±: {rec.reason}")
            print()
        
        print(f"å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")
        print()
        print("=" * 60)
        print()
    
    return recommender


if __name__ == "__main__":
    demo_local_gemma_recommender()
